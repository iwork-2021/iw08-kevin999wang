{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset. It contains translations from English to Spanish, so swap the order of the phrases. Also add `\\t` and `\\n` as the start and stop tokens in the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"\\t\"\n",
    "stop_token = \"\\n\"\n",
    "\n",
    "with open(\"data/spa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    samples = f.read().split(\"\\n\")\n",
    "\n",
    "samples = [sample.strip().split(\"\\t\")\n",
    "           for sample in samples if len(sample.strip()) > 0]\n",
    "\n",
    "samples = [(es, start_token + en + stop_token)\n",
    "           for en, es in samples if len(es) < 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99423"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ve.', '\\tGo.\\n'), ('Vete.', '\\tGo.\\n'), ('Vaya.', '\\tGo.\\n'), ('Váyase.', '\\tGo.\\n'), ('Hola.', '\\tHi.\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(samples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples, valid_samples = train_test_split(samples, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79538"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the training vocabulary. Those are the only tokens you can trust the model will know how to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 101\n",
      "Output vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "in_vocab = set()\n",
    "out_vocab = set()\n",
    "\n",
    "for in_seq, out_seq in train_samples:\n",
    "    in_vocab.update(in_seq)\n",
    "    out_vocab.update(out_seq)\n",
    "    \n",
    "in_vocab_size = len(in_vocab)\n",
    "out_vocab_size = len(out_vocab)\n",
    "print(\"Input vocab size:\", in_vocab_size)\n",
    "print(\"Output vocab size:\", out_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '°', 'º', '»', '¿', 'Á', 'É', 'Ó', 'Ú', 'á', 'è', 'é', 'í', 'ñ', 'ó', 'ö', 'ú', 'ü', 'ś', 'с', '—', '€']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '$', '%', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'á', 'ã', 'è', 'é', 'ö', '‘', '’', '₂', '€']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(out_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through validation set and remove any tokens not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_samples = []\n",
    "for in_seq, out_seq in valid_samples:\n",
    "    tmp_in_seq = [c for c in in_seq if c in in_vocab]\n",
    "    tmp_out_seq = [c for c in out_seq if c in out_vocab]\n",
    "\n",
    "    tmp_samples.append((\"\".join(tmp_in_seq), \"\".join(tmp_out_seq)))\n",
    "    \n",
    "valid_samples = tmp_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 16:24:27.277502: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-02 16:24:27.277519: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 16:24:28.034158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-02 16:24:28.034172: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-02 16:24:28.034185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (guest): /proc/driver/nvidia/version does not exist\n",
      "2022-01-02 16:24:28.034293: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "encoder_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "encoder_mask = Masking(name=\"encoder_mask\")(encoder_in)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, recurrent_dropout=0.3, name=\"encoder_lstm\")\n",
    "_, encoder_h, encoder_c = encoder_lstm(encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in = Input(shape=(None, out_vocab_size), name=\"decoder_in\")\n",
    "\n",
    "decoder_mask = Masking(name=\"decoder_mask\")(decoder_in)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.3, name=\"decoder_lstm\")\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_mask, initial_state=[encoder_h, encoder_c])\n",
    "decoder_dense = Dense(out_vocab_size, activation=\"softmax\", name=\"decoder_out\")\n",
    "decoder_out = decoder_dense(decoder_lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_in, decoder_in], decoder_out)\n",
    "seq2seq_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_in (InputLayer)        [(None, None, 101)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder_in (InputLayer)        [(None, None, 87)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_mask (Masking)         (None, None, 101)    0           ['encoder_in[0][0]']             \n",
      "                                                                                                  \n",
      " decoder_mask (Masking)         (None, None, 87)     0           ['decoder_in[0][0]']             \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        366592      ['encoder_mask[0][0]']           \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  352256      ['decoder_mask[0][0]',           \n",
      "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " decoder_out (Dense)            (None, None, 87)     22359       ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 741,207\n",
      "Trainable params: 741,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create maps to convert characters to and from ints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_token2int = {token : i for i, token in enumerate(sorted(in_vocab))}\n",
    "out_token2int = {token : i for i, token in enumerate(sorted(out_vocab))}\n",
    "out_int2token = {i : token for (token, i) in out_token2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions for one-hot encoding sequences for use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_batch_storage(batch_size, in_seq_len, out_seq_len):\n",
    "    \n",
    "    enc_in_seqs = np.zeros(\n",
    "        (batch_size, in_seq_len, in_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_in_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_out_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "        \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(samples):\n",
    "    batch_size = len(samples)\n",
    "    max_in_length = max([len(seq) for seq, _ in samples])\n",
    "    max_out_length = max([len(seq) for _, seq in samples])\n",
    "\n",
    "    enc_in_seqs, dec_in_seqs, dec_out_seqs = make_batch_storage(\n",
    "        batch_size, max_in_length, max_out_length)\n",
    "    \n",
    "    for i, (in_seq, out_seq) in enumerate(samples):\n",
    "        for time_step, token in enumerate(in_seq):\n",
    "            enc_in_seqs[i, time_step, in_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq):\n",
    "            dec_in_seqs[i, time_step, out_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq[1:]):\n",
    "            dec_out_seqs[i, time_step, out_token2int[token]] = 1\n",
    "            \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_util import Seq2SeqBatchGenerator\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = Seq2SeqBatchGenerator(train_samples, batch_size, encode_batch)\n",
    "valid_generator = Seq2SeqBatchGenerator(valid_samples, batch_size, encode_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23376/292808357.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  seq2seq_model.fit_generator(train_generator, epochs=500,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1247/1247 [==============================] - 167s 132ms/step - loss: 1.4437 - val_loss: 1.1368\n",
      "Epoch 2/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 1.1454 - val_loss: 0.9944\n",
      "Epoch 3/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 1.0435 - val_loss: 0.9361\n",
      "Epoch 4/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.9897 - val_loss: 0.8862\n",
      "Epoch 5/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.9522 - val_loss: 0.8481\n",
      "Epoch 6/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.9287 - val_loss: 0.8149\n",
      "Epoch 7/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.9114 - val_loss: 0.8188\n",
      "Epoch 8/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8989 - val_loss: 0.7839\n",
      "Epoch 9/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8884 - val_loss: 0.7694\n",
      "Epoch 10/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8821 - val_loss: 0.7580\n",
      "Epoch 11/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8783 - val_loss: 0.7526\n",
      "Epoch 12/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8708 - val_loss: 0.7536\n",
      "Epoch 13/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8678 - val_loss: 0.7518\n",
      "Epoch 14/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8652 - val_loss: 0.7603\n",
      "Epoch 15/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8618 - val_loss: 0.7680\n",
      "Epoch 16/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8604 - val_loss: 0.7282\n",
      "Epoch 17/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8575 - val_loss: 0.7568\n",
      "Epoch 18/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8616 - val_loss: 0.7318\n",
      "Epoch 19/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8596 - val_loss: 0.7294\n",
      "Epoch 20/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8582 - val_loss: 0.7430\n",
      "Epoch 21/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8606 - val_loss: 0.7260\n",
      "Epoch 22/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8622 - val_loss: 0.7384\n",
      "Epoch 23/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8628 - val_loss: 0.7298\n",
      "Epoch 24/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8626 - val_loss: 0.7191\n",
      "Epoch 25/500\n",
      "1247/1247 [==============================] - 165s 132ms/step - loss: 0.8626 - val_loss: 0.7301\n",
      "Epoch 26/500\n",
      "1247/1247 [==============================] - 164s 132ms/step - loss: 0.8627 - val_loss: 0.7192\n",
      "Epoch 27/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8615 - val_loss: 0.7169\n",
      "Epoch 28/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8650 - val_loss: 0.7136\n",
      "Epoch 29/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8684 - val_loss: 0.7267\n",
      "Epoch 30/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8700 - val_loss: 0.7293\n",
      "Epoch 31/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8736 - val_loss: 0.7392\n",
      "Epoch 32/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8741 - val_loss: 0.7435\n",
      "Epoch 33/500\n",
      "1247/1247 [==============================] - 161s 129ms/step - loss: 0.8752 - val_loss: 0.7299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3d86fa670>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "seq2seq_model.fit_generator(train_generator, epochs=500,\n",
    "                            validation_data=valid_generator,\n",
    "                            callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder/decoder models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_encoder = Model(encoder_in, [encoder_h, encoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_in (InputLayer)     [(None, None, 101)]       0         \n",
      "                                                                 \n",
      " encoder_mask (Masking)      (None, None, 101)         0         \n",
      "                                                                 \n",
      " encoder_lstm (LSTM)         [(None, 256),             366592    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 366,592\n",
      "Trainable params: 366,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dec_h_in = Input(shape=(latent_dim,), name=\"decoder_h_in\")\n",
    "inf_dec_c_in = Input(shape=(latent_dim,), name=\"decoder_c_in\")\n",
    "\n",
    "inf_dec_lstm_out, inf_dec_h_out, inf_dec_c_out = decoder_lstm(\n",
    "    decoder_in, initial_state=[inf_dec_h_in, inf_dec_c_in])\n",
    "\n",
    "inf_dec_out = decoder_dense(inf_dec_lstm_out)\n",
    "\n",
    "inf_decoder = Model(\n",
    "    [decoder_in, inf_dec_h_in, inf_dec_c_in],\n",
    "    [inf_dec_out, inf_dec_h_out, inf_dec_c_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_in (InputLayer)        [(None, None, 87)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_h_in (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_c_in (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  352256      ['decoder_in[0][0]',             \n",
      "                                 (None, 256),                     'decoder_h_in[0][0]',           \n",
      "                                 (None, 256)]                     'decoder_c_in[0][0]']           \n",
      "                                                                                                  \n",
      " decoder_out (Dense)            (None, None, 87)     22359       ['decoder_lstm[1][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 374,615\n",
      "Trainable params: 374,615\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test trained model on the first 100 samples from both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max output length:  87\n"
     ]
    }
   ],
   "source": [
    "max_out_seq_len = max([len(seq) for _, seq in samples])\n",
    "print(\"Max output length: \", max_out_seq_len)\n",
    "\n",
    "start_token_idx = out_token2int[start_token]\n",
    "stop_token_idx = out_token2int[stop_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(one_hot_seq, encoder, decoder):\n",
    "    encoding = encoder.predict(one_hot_seq)\n",
    "\n",
    "    decoder_in = np.zeros((1, 1, out_vocab_size), dtype=np.float32)\n",
    "\n",
    "    translated_text = ''\n",
    "    done_decoding = False\n",
    "    decoded_idx = start_token_idx\n",
    "    while not done_decoding:\n",
    "        decoder_in[0, 0, decoded_idx] = 1\n",
    "        decoding, h, c = decoder.predict([decoder_in] + encoding)\n",
    "        encoding = [h, c]\n",
    "        decoder_in[0, 0, decoded_idx] = 0\n",
    "\n",
    "        decoded_idx = np.argmax(decoding[0, -1, :])\n",
    "        \n",
    "        if decoded_idx == stop_token_idx:\n",
    "            done_decoding = True\n",
    "        else:\n",
    "            translated_text += out_int2token[decoded_idx]\n",
    "\n",
    "        if len(translated_text) >= max_out_seq_len:\n",
    "            done_decoding = True\n",
    "            \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: A todos nos gusta montar en bici.\n",
      "Dataset translation: \tWe all like cycling.\n",
      "\n",
      "Model output: Everyone likes a lot of the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom se rió de todos los chistes de Mary.\n",
      "Dataset translation: \tTom laughed at all of Mary's jokes.\n",
      "\n",
      "Model output: Tom got the start of Mary a dog.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom es un asqueroso.\n",
      "Dataset translation: \tTom is a creep.\n",
      "\n",
      "Model output: Tom is a good to me.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Cuál es tu meta en la vida?\n",
      "Dataset translation: \tWhat's your aim in life?\n",
      "\n",
      "Model output: What's your favorite the party?\n",
      "-----------------------------------------\n",
      "Input sentence: Ella le escucha, aunque nadie más lo haga.\n",
      "Dataset translation: \tShe listens to him even though no one else does.\n",
      "\n",
      "Model output: She lives a lot of the studing in the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Ganar o perder no es la cuestión.\n",
      "Dataset translation: \tIt doesn't matter whether you win or not.\n",
      "\n",
      "Model output: The room is a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Es un privilegio conocerte.\n",
      "Dataset translation: \tIt is a privilege to meet you.\n",
      "\n",
      "Model output: It's a book and stud.\n",
      "-----------------------------------------\n",
      "Input sentence: Disculpe, se me han caído los palillos.\n",
      "Dataset translation: \tExcuse me, I dropped a chopstick.\n",
      "\n",
      "Model output: Excuse the started is a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Hay un jardín delante de nuestra casa.\n",
      "Dataset translation: \tThere's a garden in front of our house.\n",
      "\n",
      "Model output: There is a lot of the book to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Corrí alrededor del campo.\n",
      "Dataset translation: \tI ran around the field.\n",
      "\n",
      "Model output: I really like the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Nunca me he olvidado de ustedes.\n",
      "Dataset translation: \tI've never forgotten you.\n",
      "\n",
      "Model output: I've never been to stay the party.\n",
      "-----------------------------------------\n",
      "Input sentence: A él le operaron la pierna izquierda.\n",
      "Dataset translation: \tHe had an operation on his left leg.\n",
      "\n",
      "Model output: He will be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: He hecho cosas cuestionables.\n",
      "Dataset translation: \tI've done questionable things.\n",
      "\n",
      "Model output: I've been to stay to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Soy estudiante de la universidad Hyogo.\n",
      "Dataset translation: \tI am a student at Hyogo University.\n",
      "\n",
      "Model output: I'm a big of the book to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tengo la impresión de que ella vendrá hoy.\n",
      "Dataset translation: \tI have an idea she will come today.\n",
      "\n",
      "Model output: I have a lot of the studing to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Está estudiando chino.\n",
      "Dataset translation: \tHe's studying Chinese.\n",
      "\n",
      "Model output: It's all the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Creo que va a llover hoy.\n",
      "Dataset translation: \tI think it's going to rain today.\n",
      "\n",
      "Model output: I think that we was a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Un niño conducía un rebaño de ovejas.\n",
      "Dataset translation: \tA boy was driving a flock of sheep.\n",
      "\n",
      "Model output: A book is not a book and straing.\n",
      "-----------------------------------------\n",
      "Input sentence: Estoy escribiendo una carta.\n",
      "Dataset translation: \tI'm writing a letter.\n",
      "\n",
      "Model output: I'm starting a little.\n",
      "-----------------------------------------\n",
      "Input sentence: Te llevaré a nadar.\n",
      "Dataset translation: \tI will take you for a swim.\n",
      "\n",
      "Model output: I'll get you to do the stor.\n",
      "-----------------------------------------\n",
      "Input sentence: No falta nada.\n",
      "Dataset translation: \tNothing is missing.\n",
      "\n",
      "Model output: I don't know the stud.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo le indiqué el camino.\n",
      "Dataset translation: \tI showed him the way.\n",
      "\n",
      "Model output: I found the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Qué quieres decir?\n",
      "Dataset translation: \tWhatever do you mean?\n",
      "\n",
      "Model output: What do you want to do?\n",
      "-----------------------------------------\n",
      "Input sentence: Tom necesita un cambio de decorado.\n",
      "Dataset translation: \tTom needs a change of scenery.\n",
      "\n",
      "Model output: Tom needs a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Pártelo a la mitad.\n",
      "Dataset translation: \tCut it in half.\n",
      "\n",
      "Model output: Put the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Ya decidiste qué hacer?\n",
      "Dataset translation: \tHave you decided what to do yet?\n",
      "\n",
      "Model output: Did you tell me that I want?\n",
      "-----------------------------------------\n",
      "Input sentence: El entusiasmo es contagioso.\n",
      "Dataset translation: \tEnthusiasm is contagious.\n",
      "\n",
      "Model output: The studen is a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tanto Tom como Mary estudian francés.\n",
      "Dataset translation: \tTom and Mary both study French.\n",
      "\n",
      "Model output: Tom had Mary don't want to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Me gusta oír buena música.\n",
      "Dataset translation: \tI like listening to good music.\n",
      "\n",
      "Model output: I like to be a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Muchísimas gracias por el regalo.\n",
      "Dataset translation: \tThank you very much for your gift.\n",
      "\n",
      "Model output: Thanks to be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tú estás interfiriendo.\n",
      "Dataset translation: \tYou're interfering.\n",
      "\n",
      "Model output: You're in the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Acabo de verlo.\n",
      "Dataset translation: \tI saw him just now.\n",
      "\n",
      "Model output: I've just been the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Déjame pensar esto de nuevo.\n",
      "Dataset translation: \tLet me think this over.\n",
      "\n",
      "Model output: Let me me the study.\n",
      "-----------------------------------------\n",
      "Input sentence: Quiero creer que es así.\n",
      "Dataset translation: \tI'd like to think so.\n",
      "\n",
      "Model output: I want to stay that the stud.\n",
      "-----------------------------------------\n",
      "Input sentence: Hagamos lo que dijo Tom.\n",
      "Dataset translation: \tLet's do what Tom said.\n",
      "\n",
      "Model output: Let's stay him to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom una vez trabajó en una panadería.\n",
      "Dataset translation: \tTom once worked at a bakery.\n",
      "\n",
      "Model output: Tom was a lot of the book the way.\n",
      "-----------------------------------------\n",
      "Input sentence: A ellos les gusta jugar juntos.\n",
      "Dataset translation: \tThey enjoy playing together.\n",
      "\n",
      "Model output: They like to stay a lot.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Quieres ir primero?\n",
      "Dataset translation: \tDo you want to go first?\n",
      "\n",
      "Model output: Do you want to see you a dog?\n",
      "-----------------------------------------\n",
      "Input sentence: Esperamos visitar España este verano.\n",
      "Dataset translation: \tWe are hoping to visit Spain this summer.\n",
      "\n",
      "Model output: We have a lot of the studing to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: No entiendo lo que quiere decir.\n",
      "Dataset translation: \tI don't understand what you mean.\n",
      "\n",
      "Model output: I don't have to stay that the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: Cuando el agua se congela se vuelve hielo.\n",
      "Dataset translation: \tWhen water freezes it becomes ice.\n",
      "\n",
      "Model output: When will be a lot of the book to the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Alguna vez has comido insectos?\n",
      "Dataset translation: \tHave you ever eaten insects?\n",
      "\n",
      "Model output: Have you ever been to do?\n",
      "-----------------------------------------\n",
      "Input sentence: Sal del agua.\n",
      "Dataset translation: \tGet out of the water.\n",
      "\n",
      "Model output: Get the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Él se quedó en mi hogar por tres semanas.\n",
      "Dataset translation: \tHe stayed at my place for three weeks.\n",
      "\n",
      "Model output: He took me to the start of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom hace su mejor esfuerzo.\n",
      "Dataset translation: \tTom is doing his best.\n",
      "\n",
      "Model output: Tom speaks the started to me.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: Todo el mundo estuvo de acuerdo.\n",
      "Dataset translation: \tEverybody was in agreement.\n",
      "\n",
      "Model output: Everyone like the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Si quieres tu dinero de vuelta, solo dilo.\n",
      "Dataset translation: \tIf you want your money back, just say so.\n",
      "\n",
      "Model output: If you want to see you what I was a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Nadie me dijo nada.\n",
      "Dataset translation: \tNo one said anything to me.\n",
      "\n",
      "Model output: Nobody was a lot the stor.\n",
      "-----------------------------------------\n",
      "Input sentence: A Tom le gusta estar rodeado de gente.\n",
      "Dataset translation: \tTom likes having people around.\n",
      "\n",
      "Model output: Tom likes to stay to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tu madre se encuentra en estado crítico.\n",
      "Dataset translation: \tYour mother is in critical condition.\n",
      "\n",
      "Model output: Your father is a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Hay un gato debajo de la mesa.\n",
      "Dataset translation: \tThere's a cat under the table.\n",
      "\n",
      "Model output: There is a lot of the book the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Esto no es un sueño.\n",
      "Dataset translation: \tIt isn't a dream.\n",
      "\n",
      "Model output: This isn't a good.\n",
      "-----------------------------------------\n",
      "Input sentence: Ellos son vuestros.\n",
      "Dataset translation: \tThey're yours.\n",
      "\n",
      "Model output: They're very here.\n",
      "-----------------------------------------\n",
      "Input sentence: Mañana, él alunizará.\n",
      "Dataset translation: \tTomorrow he lands on the moon.\n",
      "\n",
      "Model output: Tomorry, I was a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo jugaré contigo.\n",
      "Dataset translation: \tI'll play with you.\n",
      "\n",
      "Model output: I'll be a lot to me.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom está gordo.\n",
      "Dataset translation: \tTom is fat.\n",
      "\n",
      "Model output: Tom is here.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Puedo reservar un vuelo a Chicago?\n",
      "Dataset translation: \tCan I reserve a flight to Chicago?\n",
      "\n",
      "Model output: Can I see you a lot of the book?\n",
      "-----------------------------------------\n",
      "Input sentence: Él asistió a muchas ceremonias.\n",
      "Dataset translation: \tHe attended many ceremonies.\n",
      "\n",
      "Model output: He started the book to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Nos sentamos ahí.\n",
      "Dataset translation: \tWe sat there.\n",
      "\n",
      "Model output: We sat the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Es todo.\n",
      "Dataset translation: \tThat is all.\n",
      "\n",
      "Model output: It's happy.\n",
      "-----------------------------------------\n",
      "Input sentence: Su vida está llena de problemas.\n",
      "Dataset translation: \tHis life is full of trouble.\n",
      "\n",
      "Model output: His son is a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Qué estás esperando?\n",
      "Dataset translation: \tWhat are you waiting for?\n",
      "\n",
      "Model output: What are you so sture?\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Cómo puedes aguantarlo?\n",
      "Dataset translation: \tHow can you stand it?\n",
      "\n",
      "Model output: How can you see you a dog?\n",
      "-----------------------------------------\n",
      "Input sentence: María perdió sus lentes de lectura.\n",
      "Dataset translation: \tMary lost her reading glasses.\n",
      "\n",
      "Model output: Mary was already has a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Ahora lo digo de veras.\n",
      "Dataset translation: \tI mean it this time.\n",
      "\n",
      "Model output: I only have to go the stor.\n",
      "-----------------------------------------\n",
      "Input sentence: Ellos eran hombres conservadores.\n",
      "Dataset translation: \tThey were conservative men.\n",
      "\n",
      "Model output: They were already to see me.\n",
      "-----------------------------------------\n",
      "Input sentence: Quisiera una taza de té.\n",
      "Dataset translation: \tI'd like a cup of tea.\n",
      "\n",
      "Model output: I'd like a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Qué intenta esconder?\n",
      "Dataset translation: \tWhat are you trying to hide?\n",
      "\n",
      "Model output: What did you see the beak?\n",
      "-----------------------------------------\n",
      "Input sentence: A Tom no le gustó Mary.\n",
      "Dataset translation: \tTom didn't like Mary.\n",
      "\n",
      "Model output: Tom didn't like Mary.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Qué intentáis ocultar?\n",
      "Dataset translation: \tWhat are you trying to hide?\n",
      "\n",
      "Model output: What do you want to do?\n",
      "-----------------------------------------\n",
      "Input sentence: La táctica funcionó.\n",
      "Dataset translation: \tThe tactic worked.\n",
      "\n",
      "Model output: The book is a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Vine por ustedes.\n",
      "Dataset translation: \tI came for you.\n",
      "\n",
      "Model output: I came to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Te debo algo?\n",
      "Dataset translation: \tDo I owe you something?\n",
      "\n",
      "Model output: Do I have to do the store?\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Me ayudas a mover la mesa?\n",
      "Dataset translation: \tCan you help me move the table?\n",
      "\n",
      "Model output: Do you live the start of the party?\n",
      "-----------------------------------------\n",
      "Input sentence: Tom arriesgó su vida para salvar a Mary.\n",
      "Dataset translation: \tTom risked his life to save Mary.\n",
      "\n",
      "Model output: Tom childed the book to Mary to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: He comprado aquí durante siglos.\n",
      "Dataset translation: \tI've shopped here for ages.\n",
      "\n",
      "Model output: I've been to stay the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Até a mi perro a un árbol del jardín.\n",
      "Dataset translation: \tI tied my dog to a tree in the garden.\n",
      "\n",
      "Model output: I tried to stay a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Soy de Kioto.\n",
      "Dataset translation: \tI'm from Kyoto.\n",
      "\n",
      "Model output: I'm a litter.\n",
      "-----------------------------------------\n",
      "Input sentence: Las niñas no jugarán al tenis mañana.\n",
      "Dataset translation: \tThe girls will not play tennis tomorrow.\n",
      "\n",
      "Model output: The children doesn't know the book the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: Todas las mañanas voy de compras.\n",
      "Dataset translation: \tI go shopping every morning.\n",
      "\n",
      "Model output: Everyone seems to see the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tengo que plantar árboles en el jardín.\n",
      "Dataset translation: \tI have to plant trees in the garden.\n",
      "\n",
      "Model output: I have to go to the start of the party.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Para servir o para llevar?\n",
      "Dataset translation: \tFor here, or to go?\n",
      "\n",
      "Model output: Will you be a lot of the party?\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Cómo supiste que iba a pasar eso?\n",
      "Dataset translation: \tHow did you know that was going to happen?\n",
      "\n",
      "Model output: How did you want to stay this the way?\n",
      "-----------------------------------------\n",
      "Input sentence: Tom murió en un accidente de tráfico.\n",
      "Dataset translation: \tTom died in a traffic accident.\n",
      "\n",
      "Model output: Tom was a bad a book to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Ya no estoy casada con Tom.\n",
      "Dataset translation: \tI'm no longer married to Tom.\n",
      "\n",
      "Model output: I'm not already to Tom.\n",
      "-----------------------------------------\n",
      "Input sentence: No es un reloj.\n",
      "Dataset translation: \tIt is not a watch.\n",
      "\n",
      "Model output: It's not a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Te ayudaré si es posible.\n",
      "Dataset translation: \tI will help you if possible.\n",
      "\n",
      "Model output: I'll see you want to see the stor.\n",
      "-----------------------------------------\n",
      "Input sentence: Ella realmente quería decir el secreto.\n",
      "Dataset translation: \tShe really wanted to tell the secret.\n",
      "\n",
      "Model output: She seems that he was to stay the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Ella se tiñó de rubio.\n",
      "Dataset translation: \tShe dyed her hair blonde.\n",
      "\n",
      "Model output: She took me to here.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Has necesitado ayuda alguna vez?\n",
      "Dataset translation: \tHave you ever needed help?\n",
      "\n",
      "Model output: Have you ever seen the book?\n",
      "-----------------------------------------\n",
      "Input sentence: He venido a despedirme.\n",
      "Dataset translation: \tI've come to say goodbye.\n",
      "\n",
      "Model output: I've been to stay the sturs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: Sé que no es una broma.\n",
      "Dataset translation: \tI know it's not a joke.\n",
      "\n",
      "Model output: I know it's not a lot the stor.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo cocinaré.\n",
      "Dataset translation: \tI'll cook.\n",
      "\n",
      "Model output: I'll come.\n",
      "-----------------------------------------\n",
      "Input sentence: Fuera de mi propiedad.\n",
      "Dataset translation: \tGet off my property.\n",
      "\n",
      "Model output: Stop is the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Puedes venir un momento?\n",
      "Dataset translation: \tWould you come here a moment?\n",
      "\n",
      "Model output: Can you see you a book?\n",
      "-----------------------------------------\n",
      "Input sentence: Tom se ve aburrido.\n",
      "Dataset translation: \tTom looks bored.\n",
      "\n",
      "Model output: Tom looks the party.\n",
      "-----------------------------------------\n",
      "Input sentence: ¡Mira! Hay un avión despegando.\n",
      "Dataset translation: \tLook! There's a plane taking off.\n",
      "\n",
      "Model output: Look at a book at the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Este libro no es mío.\n",
      "Dataset translation: \tThis book isn't mine.\n",
      "\n",
      "Model output: This book is not the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom bebe.\n",
      "Dataset translation: \tTom drinks.\n",
      "\n",
      "Model output: Tom knows.\n",
      "-----------------------------------------\n",
      "Input sentence: Cuando lo oyó, le entraron ganas de llorar.\n",
      "Dataset translation: \tWhen she heard that, she felt like crying.\n",
      "\n",
      "Model output: When I was a lot of the studing to the beather.\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_util import test_predictions\n",
    "\n",
    "test_predictions(valid_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: Después de una larga espera pudimos entrar.\n",
      "Dataset translation: \tWe got in after a long wait.\n",
      "\n",
      "Model output: After is a lot of the book the way.\n",
      "-----------------------------------------\n",
      "Input sentence: Lo siento, pero es imposible.\n",
      "Dataset translation: \tI'm sorry, but it's impossible.\n",
      "\n",
      "Model output: I'm sorry, I want to stay him.\n",
      "-----------------------------------------\n",
      "Input sentence: Parecía satisfecho.\n",
      "Dataset translation: \tHe looked pleased.\n",
      "\n",
      "Model output: It seems to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Saqué el pastel del horno.\n",
      "Dataset translation: \tI took the cake out of the oven.\n",
      "\n",
      "Model output: I know him to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Es un trabajo muy difícil.\n",
      "Dataset translation: \tThat's a very tough job.\n",
      "\n",
      "Model output: It's a bad a book the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Dijiste que no entendías.\n",
      "Dataset translation: \tYou said you didn't understand.\n",
      "\n",
      "Model output: You did not see Tom the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Todo el mundo te está esperando.\n",
      "Dataset translation: \tEverybody is waiting for you.\n",
      "\n",
      "Model output: Everybody is start to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Recuerdo lo que era.\n",
      "Dataset translation: \tI remember what it was.\n",
      "\n",
      "Model output: I remember that he was stor.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom aterrizó su helicóptero sobre el techo.\n",
      "Dataset translation: \tTom landed his helicopter on the roof.\n",
      "\n",
      "Model output: Tom drested the book to the book the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom pidió direcciones.\n",
      "Dataset translation: \tTom asked for directions.\n",
      "\n",
      "Model output: Tom asked me to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Él tiene una personalidad dócil.\n",
      "Dataset translation: \tHe has a mild nature.\n",
      "\n",
      "Model output: He has a bad a book the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Hago ejercicios dos horas por día.\n",
      "Dataset translation: \tI exercise for two hours every day.\n",
      "\n",
      "Model output: I do it a lot of the book the stud.\n",
      "-----------------------------------------\n",
      "Input sentence: No me creerán aunque les jure que es cierto.\n",
      "Dataset translation: \tThey won't believe me even if I swear it is true.\n",
      "\n",
      "Model output: It won't be a lot of the book the wate.\n",
      "-----------------------------------------\n",
      "Input sentence: He dado el primer paso.\n",
      "Dataset translation: \tI've taken the first step.\n",
      "\n",
      "Model output: I've been to stay the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom y yo trabajamos juntos.\n",
      "Dataset translation: \tTom and I work together.\n",
      "\n",
      "Model output: Tom and I have a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Él es mucho mejor que tú.\n",
      "Dataset translation: \tHe is much better than you.\n",
      "\n",
      "Model output: He is my from to see me.\n",
      "-----------------------------------------\n",
      "Input sentence: Ven cualquier día que quieras.\n",
      "Dataset translation: \tCome on any day you like.\n",
      "\n",
      "Model output: Come the started that the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: El gerente dijo que era culpa tuya.\n",
      "Dataset translation: \tThe manager said it was your fault.\n",
      "\n",
      "Model output: The trush was started to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom murió feliz.\n",
      "Dataset translation: \tTom died happy.\n",
      "\n",
      "Model output: Tom did the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Le vi nuevamente.\n",
      "Dataset translation: \tI saw him again.\n",
      "\n",
      "Model output: I saw him a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Ella dejó a los niños al cuidado de su tía.\n",
      "Dataset translation: \tShe left her children in her aunt's care.\n",
      "\n",
      "Model output: She set the book to the start of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: El arte no es un lujo, sino una necesidad.\n",
      "Dataset translation: \tArt is not a luxury, but a necessity.\n",
      "\n",
      "Model output: The train is not a book a book to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Había todo tipo de actividades grupales.\n",
      "Dataset translation: \tThere were all sorts of group activities.\n",
      "\n",
      "Model output: There was a lot of the start of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Él te aconsejará al respecto.\n",
      "Dataset translation: \tHe will advise you on that matter.\n",
      "\n",
      "Model output: He will be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tú no eres japonés.\n",
      "Dataset translation: \tYou are not Japanese.\n",
      "\n",
      "Model output: You're not the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tengo que cocinar la cena hoy.\n",
      "Dataset translation: \tI have to cook dinner today.\n",
      "\n",
      "Model output: I have to go to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Soy mejor.\n",
      "Dataset translation: \tI am better.\n",
      "\n",
      "Model output: I'm my the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Te queda bien.\n",
      "Dataset translation: \tThat looks good on you.\n",
      "\n",
      "Model output: I have to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Cenaremos juntos y luego iremos al teatro.\n",
      "Dataset translation: \tWe'll dine together and then go to the theater.\n",
      "\n",
      "Model output: We started to stay a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Pasemos adentro.\n",
      "Dataset translation: \tLet's step inside.\n",
      "\n",
      "Model output: Let's stay a book.\n",
      "-----------------------------------------\n",
      "Input sentence: No me puedo costear un auto nuevo.\n",
      "Dataset translation: \tI can't afford a new car.\n",
      "\n",
      "Model output: I can't be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: «¿A qué hora os levantáis?»«A las ocho.»\n",
      "Dataset translation: \t\"What time do you guys wake up?\" \"Eight o'clock.\"\n",
      "\n",
      "Model output: \"What time do you want to stay the dood to me?\n",
      "-----------------------------------------\n",
      "Input sentence: Es un error decir mentiras.\n",
      "Dataset translation: \tIt is wrong to tell a lie.\n",
      "\n",
      "Model output: It's a bad a little.\n",
      "-----------------------------------------\n",
      "Input sentence: Ellos me mostraron muchas fotos hermosas.\n",
      "Dataset translation: \tThey showed me a lot of beautiful photos.\n",
      "\n",
      "Model output: They better the started to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Ojalá pudiera bailar todos los días.\n",
      "Dataset translation: \tI wish I could dance every day.\n",
      "\n",
      "Model output: I wish I was better to stay the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: Intenté escribir con la mano izquierda.\n",
      "Dataset translation: \tI tried writing with my left hand.\n",
      "\n",
      "Model output: I tried to stay a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo jamás te traicionaría.\n",
      "Dataset translation: \tI'd never betray you.\n",
      "\n",
      "Model output: I've been to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tomás canta bastante bien.\n",
      "Dataset translation: \tTom sings quite well.\n",
      "\n",
      "Model output: Tom can speak French.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo sé conducir un coche, pero Tom no.\n",
      "Dataset translation: \tI can drive a car, but Tom can't.\n",
      "\n",
      "Model output: I know what Tom was a book the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom se portó, para ser un principiante.\n",
      "Dataset translation: \tTom did well for a beginner.\n",
      "\n",
      "Model output: Tom bought the book to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Lo traeré de vuelta.\n",
      "Dataset translation: \tI'll bring it back.\n",
      "\n",
      "Model output: I'll be a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Casi tengo treinta años.\n",
      "Dataset translation: \tI'm almost thirty.\n",
      "\n",
      "Model output: I almost to stay the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom observa a Mary.\n",
      "Dataset translation: \tTom watches Mary.\n",
      "\n",
      "Model output: Tom worked Mary.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Quieres verlo?\n",
      "Dataset translation: \tDo you want to see it?\n",
      "\n",
      "Model output: Do you want to go the store?\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Qué tiene ella?\n",
      "Dataset translation: \tWhat does she have?\n",
      "\n",
      "Model output: What do you have a dog?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: La mayoría de la gente no lo haría así.\n",
      "Dataset translation: \tMost people wouldn't do that that way.\n",
      "\n",
      "Model output: The mother is not a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Leí el libro entero.\n",
      "Dataset translation: \tI read the entire book.\n",
      "\n",
      "Model output: I read the book the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tienes que responder a la pregunta.\n",
      "Dataset translation: \tYou need to answer the question.\n",
      "\n",
      "Model output: You must be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Has vivido aquí?\n",
      "Dataset translation: \tDid you live here?\n",
      "\n",
      "Model output: Have you ever see the sture?\n",
      "-----------------------------------------\n",
      "Input sentence: He decidido hacer eso solo.\n",
      "Dataset translation: \tI've decided to do that by myself.\n",
      "\n",
      "Model output: I've decided to stay the stud.\n",
      "-----------------------------------------\n",
      "Input sentence: Ella le vio comerse un sándwich.\n",
      "Dataset translation: \tShe saw him eating a sandwich.\n",
      "\n",
      "Model output: She saw him the book to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: No quiero esta camisa.\n",
      "Dataset translation: \tI don't want this shirt.\n",
      "\n",
      "Model output: I don't want to stay a book.\n",
      "-----------------------------------------\n",
      "Input sentence: Él se marchó hace diez minutos.\n",
      "Dataset translation: \tHe left ten minutes ago.\n",
      "\n",
      "Model output: He came me to the book the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Sabes por qué Tom vino aquí hoy?\n",
      "Dataset translation: \tDo you know the reason Tom came here today?\n",
      "\n",
      "Model output: Do you know what Tom was a lot to do?\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Me podrías pasar la sal, por favor?\n",
      "Dataset translation: \tCould you pass me the salt, please?\n",
      "\n",
      "Model output: Could you play the start the book the sturtabl?\n",
      "-----------------------------------------\n",
      "Input sentence: No le gustan las cosas dulces.\n",
      "Dataset translation: \tHe doesn't care for sweet things.\n",
      "\n",
      "Model output: I don't like the strain.\n",
      "-----------------------------------------\n",
      "Input sentence: Hoy es jueves.\n",
      "Dataset translation: \tToday is Thursday.\n",
      "\n",
      "Model output: Today's a lot.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom no se ha sentido muy bien recientemente.\n",
      "Dataset translation: \tTom hasn't been very well recently.\n",
      "\n",
      "Model output: Tom hasn't got me to stay a little.\n",
      "-----------------------------------------\n",
      "Input sentence: Todo lo que sé es que él viene de China.\n",
      "Dataset translation: \tAll I know is that he came from China.\n",
      "\n",
      "Model output: Everyone wants to stay him to see the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Dígame cuando desee hacer su orden.\n",
      "Dataset translation: \tTell me when you'd like to order.\n",
      "\n",
      "Model output: Tell me the start when I was a book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¡Qué perro más grande!\n",
      "Dataset translation: \tWhat a huge dog!\n",
      "\n",
      "Model output: What a book to do.\n",
      "-----------------------------------------\n",
      "Input sentence: Conseguimos llegar allá a tiempo.\n",
      "Dataset translation: \tWe managed to get there on time.\n",
      "\n",
      "Model output: We can start the book the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¡Vete ya!\n",
      "Dataset translation: \tGo away.\n",
      "\n",
      "Model output: Go the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Él me envió una tarjeta de cumpleaños.\n",
      "Dataset translation: \tHe sent me a birthday card.\n",
      "\n",
      "Model output: He looked a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: No te recuerdo.\n",
      "Dataset translation: \tI don't remember you.\n",
      "\n",
      "Model output: I don't like this.\n",
      "-----------------------------------------\n",
      "Input sentence: Quiero hacerlo tan a menudo como pueda.\n",
      "Dataset translation: \tI want to do that as often as I can.\n",
      "\n",
      "Model output: I want to stay a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Para qué se está escondiendo Tom?\n",
      "Dataset translation: \tWhat's Tom hiding for?\n",
      "\n",
      "Model output: What's Tom's starty?\n",
      "-----------------------------------------\n",
      "Input sentence: Me da una enchinada.\n",
      "Dataset translation: \tIt creeps me out.\n",
      "\n",
      "Model output: I have a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Tom nunca lastimaría a sus hijos.\n",
      "Dataset translation: \tTom would never hurt his children.\n",
      "\n",
      "Model output: Tom never remember the studing.\n",
      "-----------------------------------------\n",
      "Input sentence: Consiguió que la máquina funcionara.\n",
      "Dataset translation: \tHe managed to run the machine.\n",
      "\n",
      "Model output: He asked the book to be a lot.\n",
      "-----------------------------------------\n",
      "Input sentence: ¡Bienvenido a tu nuevo hogar!\n",
      "Dataset translation: \tWelcome to your new home.\n",
      "\n",
      "Model output: Welcome to stay the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Yo hice esta silla.\n",
      "Dataset translation: \tI made this chair.\n",
      "\n",
      "Model output: I did it the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Te traje café.\n",
      "Dataset translation: \tI brought you some coffee.\n",
      "\n",
      "Model output: I works to help.\n",
      "-----------------------------------------\n",
      "Input sentence: Má, ¡apúrate! Están todos esperando.\n",
      "Dataset translation: \tMom, hurry up! Everyone's waiting.\n",
      "\n",
      "Model output: Mone, I want to stay that the party.\n",
      "-----------------------------------------\n",
      "Input sentence: El viernes es cuando estoy menos ocupado.\n",
      "Dataset translation: \tFriday is when I am least busy.\n",
      "\n",
      "Model output: The same is a book to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Creo que el tren vendrá pronto.\n",
      "Dataset translation: \tI think the train will come soon.\n",
      "\n",
      "Model output: I think that is the studing.\n",
      "-----------------------------------------\n",
      "Input sentence: El sombrero estaba sucio por arriba.\n",
      "Dataset translation: \tThe hat was dirty around the top.\n",
      "\n",
      "Model output: The book is a book to the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Por favor, hable más rápido.\n",
      "Dataset translation: \tPlease speak more quickly.\n",
      "\n",
      "Model output: Please pay the book.\n",
      "-----------------------------------------\n",
      "Input sentence: El despertador sonó.\n",
      "Dataset translation: \tThe alarm went off.\n",
      "\n",
      "Model output: The problem is the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: Los ricos tienen muchos amigos.\n",
      "Dataset translation: \tThe rich have many friends.\n",
      "\n",
      "Model output: The sister is a lot of the party.\n",
      "-----------------------------------------\n",
      "Input sentence: No confíe en él.\n",
      "Dataset translation: \tDon't trust him.\n",
      "\n",
      "Model output: Don't know the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Habló del accidente?\n",
      "Dataset translation: \tDid he mention the accident?\n",
      "\n",
      "Model output: Did you speak French?\n",
      "-----------------------------------------\n",
      "Input sentence: Estoy desempleada.\n",
      "Dataset translation: \tI'm unemployed.\n",
      "\n",
      "Model output: I'm strain.\n",
      "-----------------------------------------\n",
      "Input sentence: No todo estudiante tiene un diccionario.\n",
      "Dataset translation: \tNot every student has a dictionary.\n",
      "\n",
      "Model output: Nothing is a lot of the book the beathed.\n",
      "-----------------------------------------\n",
      "Input sentence: Estoy cerrando mi tienda.\n",
      "Dataset translation: \tI'm closing my store.\n",
      "\n",
      "Model output: I'm really to see me.\n",
      "-----------------------------------------\n",
      "Input sentence: Tengo un caballo blanco.\n",
      "Dataset translation: \tI've got a white horse.\n",
      "\n",
      "Model output: I have a good a lot.\n",
      "-----------------------------------------\n",
      "Input sentence: Debemos cumplir sus órdenes.\n",
      "Dataset translation: \tWe must execute his orders.\n",
      "\n",
      "Model output: We must be a lot of the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Qué será, será.\n",
      "Dataset translation: \tWhatever will be, will be.\n",
      "\n",
      "Model output: What a lot of the sturs.\n",
      "-----------------------------------------\n",
      "Input sentence: No parece alegrarse de vernos.\n",
      "Dataset translation: \tHe doesn't look happy to see us.\n",
      "\n",
      "Model output: It seems not say that the party.\n",
      "-----------------------------------------\n",
      "Input sentence: Lo firmaré mañana.\n",
      "Dataset translation: \tI'll sign it tomorrow.\n",
      "\n",
      "Model output: I'll come to stay.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Cuál es tu equipo de fútbol favorito?\n",
      "Dataset translation: \tWhat's your favorite soccer team?\n",
      "\n",
      "Model output: What's your favorite to the book?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: Tu ayuda nos va a ahorrar mucho trabajo.\n",
      "Dataset translation: \tYour help will save us a lot of work.\n",
      "\n",
      "Model output: Her house is not a book and study to do.\n",
      "-----------------------------------------\n",
      "Input sentence: El niño se ensució las manos.\n",
      "Dataset translation: \tThe boy got his hands dirty.\n",
      "\n",
      "Model output: The boy is a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Usted debe ser el nuevo profesor.\n",
      "Dataset translation: \tYou must be the new teacher.\n",
      "\n",
      "Model output: You must be a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Quién se robó mi canasto con la carne?\n",
      "Dataset translation: \tWho stole my basket with the meat?\n",
      "\n",
      "Model output: Who took the book to the study?\n",
      "-----------------------------------------\n",
      "Input sentence: No estaba manejando tan rápido.\n",
      "Dataset translation: \tI wasn't driving all that fast.\n",
      "\n",
      "Model output: I wasn't about to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Están muy lejos.\n",
      "Dataset translation: \tThey are very far away.\n",
      "\n",
      "Model output: They're a lot.\n",
      "-----------------------------------------\n",
      "Input sentence: Ella hizo un viaje a Europa el mes pasado.\n",
      "Dataset translation: \tShe made a trip to Europe last month.\n",
      "\n",
      "Model output: She said a book a book to the book.\n",
      "-----------------------------------------\n",
      "Input sentence: No tengo tiempo para juegos.\n",
      "Dataset translation: \tI don't have time for games.\n",
      "\n",
      "Model output: I don't have a lot of the book.\n",
      "-----------------------------------------\n",
      "Input sentence: Compré esta cámara por 25.000 yenes.\n",
      "Dataset translation: \tI bought this camera for 25,000 yen.\n",
      "\n",
      "Model output: I bought Mary and start of the book.\n"
     ]
    }
   ],
   "source": [
    "test_predictions(train_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model in Core ML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_enc_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "coreml_enc_lstm = LSTM(latent_dim, return_state=True, name=\"encoder_lstm\")\n",
    "coreml_enc_out, _, _ = coreml_enc_lstm(coreml_enc_in)\n",
    "\n",
    "coreml_encoder_model = Model(coreml_enc_in, coreml_enc_out)\n",
    "coreml_encoder_model.output_layers = coreml_encoder_model._output_layers\n",
    "\n",
    "inf_encoder.save_weights(\"Es2EnCharEncoderWeights.h5\")\n",
    "coreml_encoder_model.load_weights(\"Es2EnCharEncoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools\n",
    "\n",
    "coreml_encoder = coremltools.converters.convert(\n",
    "    coreml_encoder_model,\n",
    "    input_names=\"encodedSeq\",\n",
    "    output_names=\"ignored\")\n",
    "\n",
    "coreml_encoder.save(\"Es2EnCharEncoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_dec_in = Input(shape=(None, out_vocab_size))\n",
    "\n",
    "coreml_dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "coreml_dec_lstm_out, _, _ = coreml_dec_lstm(coreml_dec_in)\n",
    "coreml_dec_dense = Dense(out_vocab_size, activation=\"softmax\")\n",
    "coreml_dec_out = coreml_dec_dense(coreml_dec_lstm_out)\n",
    "\n",
    "coreml_decoder_model = Model(coreml_dec_in, coreml_dec_out)\n",
    "coreml_decoder_model.output_layers = coreml_decoder_model._output_layers\n",
    "\n",
    "inf_decoder.save_weights(\"Es2EnCharDecoderWeights.h5\")\n",
    "coreml_decoder_model.load_weights(\"Es2EnCharDecoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 18:07:15.479782: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-01-02 18:07:15.479918: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-01-02 18:07:15.488659: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 113 nodes (60), 133 edges (69), time = 1.774ms.\n",
      "  function_optimizer: Graph size after: 113 nodes (0), 133 edges (0), time = 0.974ms.\n",
      "Optimization results for grappler item: while_body_664747\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: while_cond_664746\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2022-01-02 18:07:15.536896: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-01-02 18:07:15.537024: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-01-02 18:07:15.550228: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 71 nodes (-8), 81 edges (-11), time = 4.731ms.\n",
      "  dependency_optimizer: Graph size after: 59 nodes (-12), 65 edges (-16), time = 0.584ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.089ms.\n",
      "  constant_folding: Graph size after: 59 nodes (0), 65 edges (0), time = 1.044ms.\n",
      "  dependency_optimizer: Graph size after: 59 nodes (0), 65 edges (0), time = 0.373ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.057ms.\n",
      "Optimization results for grappler item: while_body_664747\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 0.538ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.268ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.019ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.385ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.196ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.017ms.\n",
      "Optimization results for grappler item: while_cond_664746\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.187ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.051ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.008ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.108ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.042ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
      "\n",
      "2022-01-02 18:07:15.674415: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-01-02 18:07:15.674548: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-01-02 18:07:15.682957: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 113 nodes (60), 133 edges (69), time = 1.761ms.\n",
      "  function_optimizer: Graph size after: 113 nodes (0), 133 edges (0), time = 0.959ms.\n",
      "Optimization results for grappler item: while_cond_665430\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: while_body_665431\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2022-01-02 18:07:15.731217: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-01-02 18:07:15.731348: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-01-02 18:07:15.744330: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 71 nodes (-8), 81 edges (-11), time = 4.508ms.\n",
      "  dependency_optimizer: Graph size after: 59 nodes (-12), 65 edges (-16), time = 0.611ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.072ms.\n",
      "  constant_folding: Graph size after: 59 nodes (0), 65 edges (0), time = 1.083ms.\n",
      "  dependency_optimizer: Graph size after: 59 nodes (0), 65 edges (0), time = 0.375ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.055ms.\n",
      "Optimization results for grappler item: while_cond_665430\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.2ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.051ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.008ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.107ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.042ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
      "Optimization results for grappler item: while_body_665431\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 0.524ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.214ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.018ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.409ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.194ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.017ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|█| 5/5 [00:00<00:00, 17.67 passes/\n",
      "Converting Frontend ==> MIL Ops:   0%|         | 0/61 [00:00<?, ? ops/s]\n",
      "Converting Frontend ==> MIL Ops: 100%|█| 14/14 [00:00<00:00, 71349.04 op\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|         | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|█| 40/40 [00:00<00:00, 10416.10 op\n",
      "\n",
      "Converting Frontend ==> MIL Ops: 100%|█| 14/14 [00:00<00:00, 60474.00 op\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|         | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|█| 40/40 [00:00<00:00, 11494.39 op\n",
      "Converting Frontend ==> MIL Ops: 100%|█| 61/61 [00:00<00:00, 1182.41 ops\n",
      "Running MIL Common passes: 100%|█| 33/33 [00:00<00:00, 3780.10 passes/s]\n",
      "Running MIL Clean up passes: 100%|█| 8/8 [00:00<00:00, 5077.85 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops:   0%|   | 0/80 [00:00<?, ? ops/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 1/1 [00:00<00:00, 16513.0\u001b[A\n",
      "\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 24/24 [00:00<00:00, 1255.\u001b[A\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 80/80 [00:00<00:00, 3257.\n"
     ]
    }
   ],
   "source": [
    "coreml_decoder = coremltools.converters.convert(\n",
    "    coreml_decoder_model,\n",
    "    input_names=\"encodedChar\",\n",
    "    output_names=\"nextCharProbs\")\n",
    "\n",
    "coreml_decoder.save(\"Es2EnCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert weights to 16bit floats. This shouldn't hurt performance much, if at all, and it reduces the app's download size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coremltools.models.neural_network import quantization_utils\n",
    "\n",
    "def convert_to_fp16(mlmodel_filename):\n",
    "    model_fp32 = coremltools.models.MLModel(mlmodel_filename)\n",
    "    spec_16bit = quantization_utils.quantize_weights(model_fp32, nbits=16)\n",
    "    coremltools.utils.save(spec_16bit, f\"{mlmodel_filename}16Bit.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing using linear quantization\n",
      "Quantizing layer tf_make_list_0_condition_re_initialize\n",
      "Quantizing layer tf_make_list_0_condition\n",
      "Quantizing layer model_4/encoder_lstm/PartitionedCall/while_renamed\n",
      "Quantizing layer model_4/encoder_lstm/PartitionedCall/while/while_body_663418/while/MatMul_1\n",
      "Quantizing layer model_4/encoder_lstm/PartitionedCall/while/while_body_663418/while/MatMul\n",
      "Quantizing layer model_4/encoder_lstm/PartitionedCall/while_1_condition_re_initialize\n",
      "Quantizing layer model_4/encoder_lstm/PartitionedCall/while_1_condition\n",
      "WARNING - Unable to return a quantized MLModel instance since OS is not macOS 10.14 or later. Returning a quantized model specification instead.\n",
      "Quantizing using linear quantization\n",
      "Quantizing layer tf_make_list_0_condition_re_initialize\n",
      "Quantizing layer tf_make_list_0_condition\n",
      "Quantizing layer model_5/decoder_lstm/PartitionedCall/while_renamed\n",
      "Quantizing layer model_5/decoder_lstm/PartitionedCall/while/while_body_665431/while/MatMul_1\n",
      "Quantizing layer model_5/decoder_lstm/PartitionedCall/while/while_body_665431/while/MatMul\n",
      "Quantizing layer model_5/decoder_lstm/PartitionedCall/while_1_condition_re_initialize\n",
      "Quantizing layer model_5/decoder_lstm/PartitionedCall/while_1_condition\n",
      "Quantizing layer model_5/dense/Tensordot/MatMul\n",
      "WARNING - Unable to return a quantized MLModel instance since OS is not macOS 10.14 or later. Returning a quantized model specification instead.\n"
     ]
    }
   ],
   "source": [
    "convert_to_fp16(\"Es2EnCharEncoder.mlmodel\")\n",
    "convert_to_fp16(\"Es2EnCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the maps so you can transform text to and from ints. You'll need them later in the iOS app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"esCharToInt.json\", \"w\") as f:\n",
    "    json.dump(in_token2int, f)\n",
    "with open(\"intToEnChar.json\", \"w\") as f:\n",
    "    json.dump(out_int2token, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
